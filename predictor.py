# -*- coding: utf-8 -*-
"""predictor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1riverH5QOzpO04lDEQLmsMKPUOoB4R3S
"""

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch

# Load model
print("Loading AI model...")
model_name = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Simple example
text = "Basketball player Anthony Davis "
print(f"\nStarting text: '{text}'")
print("\nGenerating word by word...\n")

# Generate 5 words, one at a time
current_text = text
for step in range(30):
    print(f"--- Step {step + 1} ---")
    print(f"Current: '{current_text}'")

    # Encode current text
    input_ids = tokenizer.encode(current_text, return_tensors="pt")

    # Get predictions
    with torch.no_grad():
        outputs = model(input_ids)
        predictions = outputs.logits

    # Get the predictions for the NEXT token
    next_token_logits = predictions[0, -1, :]
    next_token_probs = torch.softmax(next_token_logits, dim=0)

    # Get top 5 predictions
    top_probs, top_indices = torch.topk(next_token_probs, 5)

    print("Top 5 next word predictions:")
    for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):
        word = tokenizer.decode([idx])
        print(f"  {i+1}. '{word}' ({prob.item()*100:.1f}% confident)")

    # randomly choose one of the five most likely words

    sampling_probs = torch.softmax(top_probs, dim=-1)

    selected_relative_idx = torch.multinomial(sampling_probs, 1).item()

    next_token_id = top_indices[selected_relative_idx]
    next_word = tokenizer.decode([next_token_id])
    current_text += next_word


    print(f"âœ“ Chosen: '{next_word}'")
    var = {next_word}
    print (var)

    # end generation when there is a period
    if var == {'.'}:
      print(f"\nFinal generated text: '{current_text}'")
      break
    if var == {'iced'}:
      print("iced alert")
    print(f"New text: '{current_text}'\n")

print(f"\nFinal generated text: '{current_text}'")

